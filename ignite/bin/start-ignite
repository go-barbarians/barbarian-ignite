#!/usr/bin/env bash
# Copyright 2018 Barbarians.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
#
#Usage: start-ignite [OPTIONS]
# Starts an IGFS server based on the supplied options.
#     --conf_dir          The directoyr where the IGFS process will store its
#                         configuration. The default is /ignite/config.

#     --server_port       The port on which the IGFS process will listen for 
#                         requests from other servers. The 
#                         default is 10500. 

#     --log_level         The log level for the IGFS server. Either FATAL,
#                         ERROR, WARN, INFO, DEBUG. The default is INFO.

#     --zk_service        The zookeeper ensemble public uri


USER=`whoami`
HOST=`hostname -s`
DOMAIN=`hostname -d`
LOG_LEVEL=INFO
CONF_DIR="/ignite/config"
SERVER_PORT=10500

source /etc/init/aws.sh

function print_usage() {
echo "\
Usage: start-ignite [OPTIONS]
Starts an IGFS server based on the supplied options.
     --conf_dir          The directoyr where the IGFS process will store its
                         configuration. The default is /ignite/config.

     --server_port       The port on which the IGFS process will listen for 
                         requests from other servers. The 
                         default is 10500. 

     --log_level         The log level for the zookeeeper server. Either FATAL,
                         ERROR, WARN, INFO, DEBUG. The default is INFO.

     --zk_service        The zookeeper ensemble public uri
"
}

function create_ignite_logging() {
  rm -f $IGNITE_LOG_PROPS
  cp /templates/ignite/$IGNITE_LOG_PROPS.template /ignite/config/$IGNITE_LOG_PROPS
}

function create_config() {
    rm -f $IGNITE_CONF_FILE
    echo "\
<beans xmlns=\"http://www.springframework.org/schema/beans\"
       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:util=\"http://www.springframework.org/schema/util\"
       xsi:schemaLocation=\"http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans.xsd
       http://www.springframework.org/schema/util
       http://www.springframework.org/schema/util/spring-util.xsd\">

    <!--
        Optional description.
    -->
    <description>
        Spring file for Ignite node configuration with IGFS and Apache Hadoop map-reduce support enabled.
        Ignite node will start with this configuration by default.
    </description>

    <!--
        Initialize property configurer so we can reference environment variables.
    -->
    <bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\">
        <property name=\"systemPropertiesModeName\" value=\"SYSTEM_PROPERTIES_MODE_FALLBACK\"/>
        <property name=\"searchSystemEnvironment\" value=\"true\"/>
    </bean>

    <!--
        Configuration of Ignite node.
    -->
    <bean id=\"grid.cfg\" class=\"org.apache.ignite.configuration.IgniteConfiguration\">
        <property name=\"fileSystemConfiguration\">
            <list>
                <bean class=\"org.apache.ignite.configuration.FileSystemConfiguration\">
                    <!-- IGFS name you will use to access IGFS through Hadoop API. -->
                    <property name=\"name\" value=\"igfs\"/>
                    <property name=\"defaultMode\" value=\"DUAL_SYNC\" />
                    <property name=\"pathModes\">
    <map>
      <entry key=\"/tmp/.*\" value=\"PRIMARY\"/>
    </map>
  </property>
                    <!-- Configure TCP endpoint for communication with the file system instance. -->
                    <property name=\"ipcEndpointConfiguration\">
                        <bean class=\"org.apache.ignite.igfs.IgfsIpcEndpointConfiguration\">
                            <property name=\"type\" value=\"TCP\" />
                            <property name=\"host\" value=\"0.0.0.0\" />
                            <property name=\"port\" value=\"${SERVER_PORT}\" />
                        </bean>
                    </property>

                    <!--
                        Configure secondary file system if needed.
                    -->

                    <property name=\"secondaryFileSystem\">
                        <bean class=\"org.apache.ignite.hadoop.fs.IgniteHadoopIgfsSecondaryFileSystem\">
                            <property name=\"fileSystemFactory\">
                                <bean class=\"org.apache.ignite.hadoop.fs.CachingHadoopFileSystemFactory\">
                                    <property name=\"uri\" value=\"s3a://reigerstraat-central/underfs\"/>
                                    <property name=\"configPaths\" value=\"${CONF_DIR}/s3-site.xml\"/>
                                </bean>
                            </property>
                        </bean>
                    </property>
                </bean>
            </list>
        </property>

       <property name=\"discoverySpi\">
         <bean class=\"org.apache.ignite.spi.discovery.zk.ZookeeperDiscoverySpi\">
         <property name=\"zkConnectionString\" value=\"${ZK_SERVICE}\"/>
         <property name=\"sessionTimeout\" value=\"30000\"/>
         <property name=\"zkRootPath\" value=\"/apacheIgnite\"/>
         <property name=\"joinTimeout\" value=\"10000\"/>
         </bean>
       </property>

       <property name=\"memoryConfiguration\">
        <bean class=\"org.apache.ignite.configuration.MemoryConfiguration\">
            <!-- Setting a name of the default memory policy -->
            <property name=\"defaultMemoryPolicyName\" value=\"Default_Region\"/>
            <!-- Setting the page size to 4 KB -->
            <property name=\"pageSize\" value=\"4096\"/>
            <property name=\"systemCacheInitialSize\" value=\"#{512 * 1024 * 1024}\"/>
            <property name=\"systemCacheMaxSize\" value=\"#{512 * 1024 * 1024}\"/>

            <!-- Defining several memory policies for different memory regions -->
            <property name=\"memoryPolicies\">
                <list>
                    <!--
                        Default memory region that grows endlessly. A cache is bound to this memory region
                        unless it sets another one in its CacheConfiguration.
                    -->
                    <bean class=\"org.apache.ignite.configuration.MemoryPolicyConfiguration\">
                        <property name=\"name\" value=\"Default_Region\"/>
                        <!-- 100 MB memory region with disabled eviction -->
                        <property name=\"initialSize\" value=\"#{20 * 1024 * 1024}\"/>
                        <property name=\"maxSize\" value=\"#{4L * 1024 * 1024 * 1024}\"/>
                    </bean>

                </list>
            </property>
        </bean>
    </property>

</bean>
</beans>
" >> $IGNITE_CONF_FILE
    cat $IGNITE_CONF_FILE >&2
}

function create_s3_site() {
    rm -f $S3_SITE_FILE
    echo "\
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>

<configuration>

<property>
    <name>fs.s3a.metadatastore.impl</name>
    <value>org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore</value>
</property>

<property>
  <name>fs.s3a.s3guard.ddb.table</name>
  <value>my-ddb-table-name</value>
</property>

<property>
  <name>fs.s3a.s3guard.ddb.region</name>
  <value>eu-west-1</value>
</property>

<property>
  <name>fs.s3a.s3guard.ddb.table.create</name>
  <value>true</value>
</property>

<property>
  <name>fs.s3a.s3guard.ddb.table.capacity.read</name>
  <value>3</value>
</property>

<property>
  <name>fs.s3a.s3guard.ddb.table.capacity.write</name>
  <value>3</value>
</property>

<property>
  <name>fs.AbstractFileSystem.s3a.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3A</value>
</property>

<property>
  <name>fs.s3a.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>

<property>
  <name>fs.s3a.access.key</name>
  <value>$AWS_ACCESS_KEY_ID</value>
</property>

<property>
  <name>fs.s3a.secret.key</name>
  <value>$AWS_SECRET_ACCESS_KEY</value>
</property>

<property>
  <name>fs.s3a.fast.upload</name>
  <value>true</value>
</property>

<property>
  <name>fs.s3a.fast.upload.buffer</name>
  <value>disk</value>
</property>

<property>
  <name>fs.s3a.multipart.size</name>
  <value>100M</value>
</property>

<property>
  <name>fs.s3a.fast.upload.active.blocks</name>
  <value>8</value>
</property>

<property>
  <name>fs.s3a.multipart.purge</name>
  <value>true</value>
</property>

<property>
  <name>ireland.endpoint</name>
  <value>s3-eu-west-1.amazonaws.com</value>
</property>

<property>
  <name>fs.s3a.endpoint</name>
  <value>\${ireland.endpoint}</value>
</property>

<property>
  <name>fs.s3a.connection.ssl.enabled</name>
  <value>true</value>
</property>

<property>
  <name>fs.s3a.multipart.purge.age</name>
  <value>86400</value>
</property>

<property>
  <name>fs.s3a.buffer.dir</name>
  <value>/tmp</value>
</property>

<property>
  <name>fs.s3a.aws.credentials.provider</name>
  <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
</property>
</configuration>
" >> $S3_SITE_FILE
    cat $S3_SITE_FILE >&2
}

optspec=":hv-:"
while getopts "$optspec" optchar; do

    case "${optchar}" in
        -)
            case "${OPTARG}" in
                conf_dir=*)
                    CONF_DIR=${OPTARG##*=}
                    ;;
                server_port=*)
                    SERVER_PORT=${OPTARG##*=}
                    ;;
                zk_service=*)
                    ZK_SERVICE=${OPTARG##*=}
                    ;;
                log_level=*)
                    LOG_LEVEL=${OPTARG##*=}
                    ;;
                *)
                    echo "Unknown option --${OPTARG}" >&2
                    exit 1
                    ;;
            esac;;
        h)
            print_usage
            exit
            ;;
        v)
            echo "Parsing option: '-${optchar}'" >&2
            ;;
        *)
            if [ "$OPTERR" != 1 ] || [ "${optspec:0:1}" = ":" ]; then
                echo "Non-option argument: '-${OPTARG}'" >&2
            fi
            ;;
    esac
done

IGNITE_LOG_PROPS="java.util.logging.properties"
IGNITE_CONF_FILE="$CONF_DIR/default-config.xml"
S3_SITE_FILE="$CONF_DIR/s3-site.xml"

export HADOOP_CLASSPATH=/ignite/config:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/tools/*:hadoop/share/hadoop/tools/lib/*
export IGNITE_CUSTOM_CLASSPATH=$HADOOP_CLASSPATH

create_ignite_logging && create_s3_site && create_config && exec /ignite/bin/ignite.sh -v
